{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fast_Api.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBO_aM-DolTK"
      },
      "source": [
        "!pip install transformers==3\n",
        "!pip install cloud-tpu-client==0.10 https://storage.googleapis.com/tpu-pytorch/wheels/torch_xla-1.8.1-cp37-cp37m-linux_x86_64.whl\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "!pip install fastapi\n",
        "!pip install pyngrok\n",
        "!pip install uvicorn\n",
        "\n",
        "from fastapi import FastAPI\n",
        "import nest_asyncio\n",
        "from pyngrok import ngrok\n",
        "import uvicorn\n",
        "from transformers import RobertaModel, RobertaTokenizer\n",
        "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
        "from keras.utils.np_utils import to_categorical\n",
        "# from keras.utils import to_categorical\n",
        "from keras.preprocessing.sequence import pad_sequences as pad\n",
        "\n",
        "import torch_xla.core.xla_model as xm\n",
        "\n",
        "EPOCHS = 20\n",
        "SPLIT = 0.8\n",
        "MAXLEN = 48\n",
        "DROP_RATE = 0.3\n",
        "np.random.seed(42)\n",
        "\n",
        "OUTPUT_UNITS = 3\n",
        "BATCH_SIZE = 384\n",
        "LR = (4e-5, 1e-2)\n",
        "ROBERTA_UNITS = 768\n",
        "VAL_BATCH_SIZE = 384\n",
        "MODEL_SAVE_PATH = 'sentiment_model.pt'\n",
        "model = 'roberta-base'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoJ-mkE4DC-W"
      },
      "source": [
        "Google Drive was used to mount and get sentiment_model.pt file of the previous running algorithm. Since model file was too big and it takes much time to upload we have tested code one time and we have commented out now because of separate drives"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kMu1NCz5rSU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ef40d2ad-d17e-40fe-9063-92eb858763a8"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oq-6pak6osjo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2758a2cf-a893-4ad0-c3c7-f5525adaa678"
      },
      "source": [
        "import torch.nn as nn\n",
        "import torch\n",
        "\n",
        "class Roberta(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Roberta, self).__init__()\n",
        "        self.softmax = nn.Softmax(dim=1)\n",
        "        self.drop = nn.Dropout(DROP_RATE)\n",
        "        \n",
        "        self.roberta = RobertaModel.from_pretrained(model)\n",
        "        self.dense = nn.Linear(ROBERTA_UNITS, OUTPUT_UNITS)\n",
        "        \n",
        "        \n",
        "    def forward(self, inp, att):\n",
        "        inp = inp.view(-1, MAXLEN)\n",
        "        _, self.feat = self.roberta(inp, att)\n",
        "        return self.softmax(self.dense(self.drop(self.feat)))\n",
        "\n",
        "network = Roberta()\n",
        "network.load_state_dict(torch.load('/content/drive/MyDrive/DSPD/dspd_sentiment_analysis.pt'))\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aBWXydAEFlOX"
      },
      "source": [
        "FAST API was deployed on google COLAB because of torch xla version, torch xla is not supported on local windows as we tried to integrate with local cpu settings."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zh81dhoj7NOt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47ba8356-c7b6-47c9-e2a0-3996033253f1"
      },
      "source": [
        "\n",
        "app = FastAPI()\n",
        "\n",
        "\n",
        "device = xm.xla_device()\n",
        "network = network.to(device)\n",
        "tokenizer =  AutoTokenizer.from_pretrained(model, use_fast=True)\n",
        "def predict_sentiment(tweet):\n",
        "    pg, tg = 'post', 'post'\n",
        "    tweet_ids = tokenizer.encode(tweet.strip())\n",
        "    sent = {0: 'positive', 1: 'neutral', 2: 'negative'}\n",
        "\n",
        "    att_mask_idx = len(tweet_ids) - 1\n",
        "    if 0 not in tweet_ids: tweet_ids = 0 + tweet_ids\n",
        "    tweet_ids = pad([tweet_ids], maxlen=MAXLEN, value=1, padding=pg, truncating=tg)\n",
        "    # device = xm.xla_device()\n",
        "    att_mask = np.zeros(MAXLEN)\n",
        "    att_mask[1:att_mask_idx] = 1\n",
        "    att_mask = att_mask.reshape((1, -1))\n",
        "    if 2 not in tweet_ids: tweet_ids[-1], att_mask[-1] = 2, 0\n",
        "    tweet_ids, att_mask = torch.LongTensor(tweet_ids), torch.LongTensor(att_mask)\n",
        "    return sent[np.argmax(network.forward(tweet_ids.to(device), att_mask.to(device)).detach().cpu().numpy())]\n",
        "\n",
        "#Default sentiment\n",
        "@app.get(\"/\")\n",
        "def read_root():\n",
        "  return predict_sentiment(\"I dont like this\")\n",
        "\n",
        "#parametric input i.e *.io/i like this\n",
        "@app.get(\"/{sentiment}\")\n",
        "def predict_Sentiment(sentiment: str):\n",
        "  vara = predict_sentiment(sentiment)\n",
        "  return vara\n",
        "\n",
        "ngrok_tunnel = ngrok.connect(8000)\n",
        "print('Public URL:', ngrok_tunnel.public_url)\n",
        "nest_asyncio.apply()\n",
        "uvicorn.run(app, port=8000)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Public URL: http://36aed55be163.ngrok.io\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Started server process [58]\n",
            "INFO:uvicorn.error:Started server process [58]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:uvicorn.error:Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:uvicorn.error:Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "INFO:uvicorn.error:Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     175.107.213.39:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     175.107.213.39:0 - \"GET /favicon.ico HTTP/1.1\" 200 OK\n",
            "INFO:     175.107.213.39:0 - \"GET /I%20want%20to%20know%20more%20about%20your%20product. HTTP/1.1\" 200 OK\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "INFO:     Shutting down\n",
            "INFO:uvicorn.error:Shutting down\n",
            "INFO:     Waiting for application shutdown.\n",
            "INFO:uvicorn.error:Waiting for application shutdown.\n",
            "INFO:     Application shutdown complete.\n",
            "INFO:uvicorn.error:Application shutdown complete.\n",
            "INFO:     Finished server process [58]\n",
            "INFO:uvicorn.error:Finished server process [58]\n"
          ],
          "name": "stderr"
        }
      ]
    }
  ]
}